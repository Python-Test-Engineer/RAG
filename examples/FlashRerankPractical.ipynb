{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sunnysavita10/Generative-AI-Indepth-Basic-to-Advance/blob/main/FlashRerankPractical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HTzTeXkt-Ny"
   },
   "source": [
    "**Model Options:**\n",
    "- **Nano**: ~4MB, blazing fast model with competitive performance (ranking precision).\n",
    "- **Small**: ~34MB, slightly slower with the best performance (ranking precision).\n",
    "- **Medium**: ~110MB, slower model with the best zero-shot performance (ranking precision).\n",
    "- **Large**: ~150MB, slower model with competitive performance (ranking precision) for 100+ languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBpXU2_Mt_-T"
   },
   "source": [
    " **Flash Rank**: Ultra-lite & Super-fast Python library for search & retrieval re-ranking.\n",
    "\n",
    "- **Ultra-lite**: No heavy dependencies. Runs on CPU with a tiny ~4MB reranking model.\n",
    "- **Super-fast**: Speed depends on the number of tokens in passages and query, plus model depth.\n",
    "- **Cost-efficient**: Ideal for serverless deployments with low memory and time requirements.\n",
    "- **Based on State-of-the-Art Cross-encoders**: Includes models like ms-marco-TinyBERT-L-2-v2 (default), ms-marco-MiniLM-L-12-v2, rank-T5-flan, and ms-marco-MultiBERT-L-12.\n",
    "- **Sleek Models for Efficiency**: Designed for minimal overhead in user-facing scenarios.\n",
    "\n",
    "_Flash Rank is tailored for scenarios requiring efficient and effective reranking, balancing performance with resource usage._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5d9ptq9tOLg",
    "outputId": "eac0f366-39d0-4de1-fc86-297fed840821"
   },
   "outputs": [],
   "source": [
    "!pip install flashrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSjnmXLbuKhV"
   },
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NgkbKGBunFn"
   },
   "outputs": [],
   "source": [
    "query = \"How to speedup LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxFLnmwwunnG"
   },
   "outputs": [],
   "source": [
    "passages = [\n",
    "   {\n",
    "      \"id\":1,\n",
    "      \"text\":\"Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.\",\n",
    "      \"meta\": {\"additional\": \"info1\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":2,\n",
    "      \"text\":\"LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper\",\n",
    "      \"meta\": {\"additional\": \"info2\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":3,\n",
    "      \"text\":\"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods Iâ€™ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
    "      \"meta\": {\"additional\": \"info3\"}\n",
    "\n",
    "   },\n",
    "   {\n",
    "      \"id\":4,\n",
    "      \"text\":\"Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.\",\n",
    "      \"meta\": {\"additional\": \"info4\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":5,\n",
    "      \"text\":\"vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels\",\n",
    "      \"meta\": {\"additional\": \"info5\"}\n",
    "   }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Vp8evUat4Yf"
   },
   "outputs": [],
   "source": [
    "from flashrank.Ranker import Ranker, RerankRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEKdOyY9uGQ8"
   },
   "outputs": [],
   "source": [
    "def get_result(query,passages,choice):\n",
    "  if choice == \"Nano\":\n",
    "    ranker = Ranker()\n",
    "  elif choice == \"Small\":\n",
    "    ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
    "  elif choice == \"Medium\":\n",
    "    ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
    "  elif choice == \"Large\":\n",
    "    ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
    "  rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "  results = ranker.rerank(rerankrequest)\n",
    "  print(results)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7juQN4HX4y5p",
    "outputId": "e5049c03-79af-4a68-f5c4-96f2f3677482"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"sunny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90z7C8sTuU-y",
    "outputId": "30ecc61b-9024-43db-9e3c-c9c50d5d239e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GS5ndB7kusz2",
    "outputId": "7631bffb-8f85-45dd-f845-f5eba41e2d4b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xaL3dXaxnd2",
    "outputId": "91dfb259-cdc2-49fa-8ca3-0ae635419f38"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1-4gQdHuy8U",
    "outputId": "05a1d225-f51f-439e-fe48-73bac2465796"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PXvV0Itu0rh",
    "outputId": "27e278fe-a8a9-497d-ea95-c503a919e2fd"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzofq9Fou3RQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlfpkIBdu4mf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mub11J8gu6mm"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXqPAq0QvHZH"
   },
   "outputs": [],
   "source": [
    "documents = TextLoader(\"/content/state_of_the_union.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aQEhuJsvmB2"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7eg9FN6voFb"
   },
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2aGoUMAvqZw"
   },
   "outputs": [],
   "source": [
    "for id, text in enumerate(texts):\n",
    "    text.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZ8dATGS6TVn",
    "outputId": "035cc344-6397-42a1-acf6-f0f44b8c1e98"
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5T-mJxtvsNw",
    "outputId": "628821ca-dd80-4374-c0a4-0ed1dc85fdf5"
   },
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVJ52yXjyWs_",
    "outputId": "aa4cc7fd-9046-48b1-b350-8f183505a6cf"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRMhm3DjvtkX"
   },
   "outputs": [],
   "source": [
    "retriever = FAISS.from_documents(texts, embedding).as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_6NVzg-vvpV"
   },
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O3Zt_4kvxWX"
   },
   "outputs": [],
   "source": [
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d65muVfQvyop",
    "outputId": "23fa1ba2-0b55-4d1c-8632-ccd46a275ae5"
   },
   "outputs": [],
   "source": [
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvrnj0O0v0Pe"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN6kmZ6Rv2VA"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hrg7KvbCv630",
    "outputId": "0209aac4-08d4-490e-810c-d9459b6a804b"
   },
   "outputs": [],
   "source": [
    "compressor = FlashrankRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cugusxEgv9E3"
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Pu1osglv_aY"
   },
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever.invoke(\"What did the president say about Ketanji Jackson Brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YgfzFuB7TpM",
    "outputId": "57977d13-2bf5-420f-8bfb-7742853cfcc2"
   },
   "outputs": [],
   "source": [
    "len(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVQiO2vY7nuT",
    "outputId": "85bad12f-0e9e-446f-e686-ae3d2629076f"
   },
   "outputs": [],
   "source": [
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iC6mJMISwALJ",
    "outputId": "d6e30499-492f-43cd-9ebe-6580ba292cc3"
   },
   "outputs": [],
   "source": [
    "print([doc.metadata[\"id\"] for doc in compressed_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmHurSMPwDgT",
    "outputId": "e6d72721-d8cd-4e07-8987-6c67b6e5a963"
   },
   "outputs": [],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAgLxoK2wFCm"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHPoBgKqwFwl",
    "outputId": "9d6c9d03-a052-4c0c-a9d2-825d407e0e27"
   },
   "outputs": [],
   "source": [
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x3PiL69yf0V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBiDPCIpoyGNT1WYhF/3UL",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
