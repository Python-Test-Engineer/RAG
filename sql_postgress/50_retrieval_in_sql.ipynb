{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from tqdm import tqdm\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dev.to/stephenc222/how-to-use-postgresql-to-store-and-query-vector-embeddings-h4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.utilities.sql_database.SQLDatabase object at 0x00000182523D6330>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrcra\\Desktop\\RAG\\venv\\Lib\\site-packages\\langchain_community\\utilities\\sql_database.py:123: SAWarning: Did not recognize type 'vector' of column 'embedding'\n",
      "  self._metadata.reflect(\n"
     ]
    }
   ],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "CONNECTION_STRING = \"postgresql+psycopg2://postgres:postgres@localhost:5432/postgres\"  # Replace with your own\n",
    "db = SQLDatabase.from_uri(CONNECTION_STRING)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the semantic search running the following query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_title = embeddings_model.embed_query(\"hope about the future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeded_title))\n",
    "print(embeded_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    return db.get_table_info()\n",
    "\n",
    "\n",
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = get_schema(None)\n",
    "# print(schema)\n",
    "with open(\"schema.sql\", \"w\") as f:\n",
    "    f.write(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a Postgres expert. Given an input question, first create a syntactically correct Postgres query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per Postgres. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n",
    "\n",
    "You can use an extra extension which allows you to run semantic similarity using <-> operator on tables containing columns named \"embeddings\".\n",
    "<-> operator can ONLY be used on embeddings columns.\n",
    "The embeddings value for a given row typically represents the semantic meaning of that row.\n",
    "The vector represents an embedding representation of the question, given below. \n",
    "Do NOT fill in the vector values directly, but rather specify a `[search_word]` placeholder, which should contain the word that would be embedded for filtering.\n",
    "For example, if the user asks for songs about 'the feeling of loneliness' the query could be:\n",
    "'SELECT \"[whatever_table_name]\".\"SongName\" FROM \"[whatever_table_name]\" ORDER BY \"embeddings\" <-> '[loneliness]' LIMIT 5'\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: <Question here>\n",
    "SQLQuery: <SQL Query to run>\n",
    "SQLResult: <Result of the SQLQuery>\n",
    "Answer: <Final answer here>\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{schema}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", template), (\"human\", \"{question}\")]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can create the chain using **[LangChain Expression Language](https://python.langchain.com/docs/expression_language/)**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "db = SQLDatabase.from_uri(\n",
    "    CONNECTION_STRING\n",
    ")  # We reconnect to db so the new columns are loaded as well.\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "sql_query_chain = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | llm.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Which are the 5 rock songs with titles about deep feeling of dispair?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain simply generates the query. Now we will create the full chain that also handles the execution and the final result for the user:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def replace_brackets(match):\n",
    "    words_inside_brackets = match.group(1).split(\", \")\n",
    "    embedded_words = [\n",
    "        str(embeddings_model.embed_query(word)) for word in words_inside_brackets\n",
    "    ]\n",
    "    return \"', '\".join(embedded_words)\n",
    "\n",
    "\n",
    "def get_query(query):\n",
    "    sql_query = re.sub(r\"\\[([\\w\\s,]+)\\]\", replace_brackets, query)\n",
    "    return sql_query\n",
    "\n",
    "\n",
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", template), (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "full_chain = (\n",
    "    RunnablePassthrough.assign(query=sql_query_chain)\n",
    "    | RunnablePassthrough.assign(\n",
    "        schema=get_schema,\n",
    "        response=RunnableLambda(lambda x: db.run(get_query(x[\"query\"]))),\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Chain\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Filtering a column based on semantic meaning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to retrieve songs that express `deep feeling of dispair`, but filtering based on genre:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Which are the 5 rock songs with titles about deep feeling of dispair?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is substantially different in implementing this method is that we have combined:\n",
    "\n",
    "- Semantic search (songs that have titles with some semantic meaning)\n",
    "- Traditional tabular querying (running JOIN statements to filter track based on genre)\n",
    "\n",
    "This is something we _could_ potentially achieve using metadata filtering, but it's more complex to do so (we would need to use a vector database containing the embeddings, and use metadata filtering based on genre).\n",
    "\n",
    "However, for other use cases metadata filtering **wouldn't be enough**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Combining filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"I want to know the 3 albums which have the most amount of songs in the top 150 saddest songs\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have result for 3 albums with most amount of songs in top 150 saddest ones. This **wouldn't** be possible using only standard metadata filtering. Without this _hybdrid query_, we would need some postprocessing to get the result.\n",
    "\n",
    "Another similar exmaple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"I need the 6 albums with shortest title, as long as they contain songs which are in the 20 saddest song list.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the query looks like to double check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    sql_query_chain.invoke(\n",
    "        {\n",
    "            \"question\": \"I need the 6 albums with shortest title, as long as they contain songs which are in the 20 saddest song list.\"\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Combining two separate semantic searches\n",
    "\n",
    "One interesting aspect of this approach which is **substantially different from using standar RAG** is that we can even **combine** two semantic search filters:\n",
    "\n",
    "- _Get 5 saddest songs..._\n",
    "- _**...obtained from albums with \"lovely\" titles**_\n",
    "\n",
    "This could generalize to **any kind of combined RAG** (paragraphs discussing _X_ topic belonging from books about _Y_, replies to a tweet about _ABC_ topic that express _XYZ_ feeling)\n",
    "\n",
    "We will combine semantic search on songs and album titles, so we need to do the same for `Album` table:\n",
    "\n",
    "1. Generate the embeddings\n",
    "2. Add them to the table as a new column (which we need to add in the table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.run('ALTER TABLE \"Album\" ADD COLUMN \"embeddings\" vector;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 226.07it/s]\n"
     ]
    }
   ],
   "source": [
    "songs = [\"A sad song\", \"A happy song\", \"A song about joy\", \"A song about sadness\"]\n",
    "song_embeddings = embeddings_model.embed_documents(songs)\n",
    "for i in tqdm(range(len(song_embeddings))):\n",
    "    song = songs[i].replace(\"'\", \"''\")\n",
    "    song_embedding = song_embeddings[i]\n",
    "    sql_command = f'INSERT INTO   \"tracks\" (\"track\", \"embeddings\") VALUES (\\'{song}\\', ARRAY{song_embedding})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\mrcra\\Desktop\\RAG\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 2\u001b[1;36m\n\u001b[1;33m    album_titles = [title[0] for title in eval(albums)]\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "albums = db.run('SELECT \"track\" FROM \"tracks\"')\n",
    "album_titles = [title[0] for title in eval(albums)]\n",
    "album_title_embeddings = embeddings_model.embed_documents(album_titles)\n",
    "for i in tqdm(range(len(album_title_embeddings))):\n",
    "    album_title = album_titles[i].replace(\"'\", \"''\")\n",
    "    album_embedding = album_title_embeddings[i]\n",
    "    sql_command = (\n",
    "        f'UPDATE \"tracks\" SET \"embeddings\" = ARRAY{album_embedding} WHERE \"track\" ='\n",
    "        + f\"'{album_title}'\"\n",
    "    )\n",
    "    db.run(sql_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeded_title = embeddings_model.embed_query(\"hope about the future\")\n",
    "query = (\n",
    "    'SELECT \"track\".\"Title\" FROM \"tracks\" WHERE \"embeddings\" IS NOT NULL ORDER BY \"embeddings\" <-> '\n",
    "    + f\"'{embeded_title}' LIMIT 5\"\n",
    ")\n",
    "db.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine both filters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\n",
    "    CONNECTION_STRING\n",
    ")  # We reconnect to dbso the new columns are loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"I want to know songs about breakouts obtained from top 5 albums about love\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something **different** that **couldn't be achieved** using standard metadata filtering over a vectordb.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
