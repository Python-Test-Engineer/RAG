{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE items (id bigserial PRIMARY KEY, content TEXT, embedding vector(1536));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-4_jr\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Set up your OpenAI API key\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "client = OpenAI()\n",
    "# Choose a model\n",
    "model = \"text-embedding-ada-002\"\n",
    "print(OPENAI_API_KEY[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PostgreSQL...\n",
      "Successfully connected to PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to PostgreSQL...\")\n",
    "conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\",\n",
    "    host=\"host.docker.internal\",\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Successfully connected to PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the prompt to the pgvector embedding\n",
    "def get_embedding(prompt):\n",
    "    response = openai.embeddings.create(input=prompt, model=\"text-embedding-ada-002\")\n",
    "\n",
    "    embedding = response.data[0].embedding\n",
    "\n",
    "    # Converting the embedding to the pgvector and returning it\n",
    "    return \"[\" + \",\".join(map(str, embedding)) + \"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, find the most relevant movies for a provided user prompt by calculating the cosine distance (`<=>`) between the prompt's and movies' embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is out of the box technology\n",
      "Most similar sentences:\n",
      "ID: cbb00b10741723c0e1aa057766000932,  Cos: -0.008235746402816435, CONTENT: But one size doesn’t fit all for search. You can utilize out-of-the-box technology, build your own with feature-rich, custom design and functionality, or anything in-between.,\n",
      "ID: 1e10486a3f5515298c1e0cdbdebe41db,  Cos: -0.01675345808164619, CONTENT: Discover the blueprint for planning, designing, and building a search experience that meets your users’ needs, your team’s resources, and (of course) your budget. With an overview of the latest tech and real-world case studies that highlight what’s possible, you can envision your future search experience and bring it to life.,\n",
      "ID: c2d7a7bc292ea16bd674aa1039a2aa15,  Cos: -0.06120557004047611, CONTENT: There’s never been a better time to create exceptional search experiences. By leveraging the capabilities of LLMs and generative AI, we can predict user intent, improve relevance, surface timely content, and even provide human-like responses.,\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    query = \"What is out of the box technology\"\n",
    "\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    # Perform a cosine similarity search\n",
    "    sql = \"\"\"\n",
    "        SELECT element_id, element_text, 1 - (embedding <=> %s) AS cosine_similarity\n",
    "        FROM tbl_doc_elements\n",
    "        WHERE element_type = 'NARRATIVETEXT'\n",
    "        ORDER BY cosine_similarity DESC LIMIT 3;\n",
    "        \"\"\"\n",
    "    cur.execute(sql, (query_embedding,))\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "\n",
    "    print(\"Most similar sentences:\")\n",
    "\n",
    "    for row in cur.fetchall():\n",
    "\n",
    "        print(f\"ID: {row[0]},  Cos: {row[2]}, CONTENT: {row[1]},\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"Error executing query\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
