{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from uuid import uuid4\n",
    "from random import randint\n",
    "from utils.get_openai_api_key import get_test_key\n",
    "from utils.get_postgres_connection import _conn_open\n",
    "from utils.load_json import load_json\n",
    "from rich.console import Console\n",
    "import psycopg2\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting to PostgreSQL...\")\n",
    "conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\",\n",
    "    host=\"localhost\",\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Successfully connected to PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting number of characters in text field...\")\n",
    "id = \"02702276-19b3-438c-82cf-3c3bf5a95228\"\n",
    "sql = f\"\"\"\n",
    "SELECT SUM(LENGTH(element_text)) \n",
    "FROM tbl_doc_elements\n",
    "WHERE element_type IN ('NARRATIVETEXT', 'TITLE', 'SUBTITLE','HEADER', 'FOOTER') \n",
    "AND id = '{id}';\n",
    "\"\"\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -c \"import nltk; nltk.download('punkt')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input text to be summarized\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. It is a multidisciplinary field that combines computer science, linguistics, and cognitive psychology to enable computers to process, understand, and generate natural language data. NLP has many applications, including language translation, sentiment analysis, text summarization, and speech recognition. Some of the key challenges in NLP include dealing with ambiguity, context, and common sense. Despite these challenges, NLP has made significant progress in recent years, with the development of new algorithms and techniques such as deep learning and word embeddings.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957329c45c9c48b683b3a4413b9bee7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\Desktop\\RAG\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\owner\\.cache\\huggingface\\hub\\models--bert-large-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5e8780021e4e45ad7d91e412563a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5682316fa3b247e283183f443283627a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfa12fae0934ac9a854f3bb91c89012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270c998b3aee43c98be6c3f757d6b040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. It is a multidisciplinary field that combines computer science, linguistics, and cognitive psychology to enable computers to process, understand, and generate natural language data. NLP has many applications, including language translation, sentiment analysis, text summarization, and speech recognition. Some of the key challenges in NLP include dealing with ambiguity, context, and common sense. Despite these challenges, NLP has made significant progress in recent years, with the development of new algorithms and techniques such as deep learning and word embeddings.\n",
      "\n",
      "\n",
      "Summary:\n",
      "NLP has many applications, including language translation, sentiment analysis, text summarization, and speech recognition. Some of the key challenges in NLP include dealing with ambiguity, context, and common sense.\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "# Input text to be summarized\n",
    "\n",
    "\n",
    "# Create a BERT extractive summarizer\n",
    "summarizer = Summarizer()\n",
    "\n",
    "# Generate the summary\n",
    "summary = summarizer(\n",
    "    text, min_length=50, max_length=150\n",
    ")  # You can adjust the min_length and max_length parameters\n",
    "\n",
    "# Output the summary\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "The above example defines a User model with fields for age, password, and username. A custom validation rule that verifies the password field has at least eight characters in length is added using the validator decorator. The User model receives the incoming data, and Pydantic handles the validation automatically. The validation process ends, and an error is raised if the data is invalid.\n",
      "\n",
      "You must note that the data structure is defined by Pydantic using type hints; the expected data types for the age, password, and username fields are indicated by the str and int type hints, respectively. Pydantic will automatically verify that the incoming data satisfies these requirements and raise errors if any data types are incorrect.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summary:\n",
      "The above example defines a User model with fields for age, password, and username. Pydantic will automatically verify that the incoming data satisfies these requirements and raise errors if any data types are incorrect.\n"
     ]
    }
   ],
   "source": [
    "# Create a BERT extractive summarizer\n",
    "summarizer = Summarizer()\n",
    "text = \"\"\"\n",
    "The above example defines a User model with fields for age, password, and username. A custom validation rule that verifies the password field has at least eight characters in length is added using the validator decorator. The User model receives the incoming data, and Pydantic handles the validation automatically. The validation process ends, and an error is raised if the data is invalid.\n",
    "\n",
    "You must note that the data structure is defined by Pydantic using type hints; the expected data types for the age, password, and username fields are indicated by the str and int type hints, respectively. Pydantic will automatically verify that the incoming data satisfies these requirements and raise errors if any data types are incorrect.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Generate the summary\n",
    "summary = summarizer(\n",
    "    text, min_length=50, max_length=150\n",
    ")  # You can adjust the min_length and max_length parameters\n",
    "\n",
    "# Output the summary\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(sql)\n",
    "num_chars = cur.fetchone()[0]\n",
    "print(f\"Num in text field {id[:8]}... is {num_chars:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
